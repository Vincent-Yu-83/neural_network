{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","class MLP(torch.nn.Module):\n","    # 用模型参数声明层。这里，我们声明两个全连接的层\n","    def __init__(self):\n","        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n","        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n","        super().__init__()\n","        self.hidden = torch.nn.Linear(20, 256)  # 隐藏层\n","        self.out = torch.nn.Linear(256, 10)  # 输出层\n","\n","    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n","    def forward(self, X):\n","        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n","        return self.out(F.relu(self.hidden(X)))\n","\n","class MySequential(torch.nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        for idx, module in enumerate(args):\n","            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n","            # 变量_modules中。_module的类型是OrderedDict\n","            self._modules[str(idx)] = module\n","\n","    def forward(self, X):\n","        # OrderedDict保证了按照成员添加的顺序遍历它们\n","        for block in self._modules.values():\n","            X = block(X)\n","        return X\n","    \n","class FixedHiddenMLP(torch.nn.Module):\n","    '''\n","        在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。\n","\n","        注意，在返回输出之前，模型做了一些不寻常的事情： 它运行了一个while循环，在L1范数大于1的条件下， 将输出向量除以2，直到它满足条件为止。 \n","        最后，模型返回了X中所有项的和。 注意，此操作可能不会常用于在任何实际任务中， 我们只展示如何将任意代码集成到神经网络计算的流程中。\n","    '''\n","    def __init__(self):\n","        super().__init__()\n","        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n","        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n","        self.linear = torch.nn.Linear(20, 20)\n","\n","    def forward(self, X):\n","        X = self.linear(X)\n","        # 使用创建的常量参数以及relu和mm函数\n","        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n","        # 复用全连接层。这相当于两个全连接层共享参数\n","        X = self.linear(X)\n","        # 控制流\n","        while X.abs().sum() > 1:\n","            X /= 2\n","        return X.sum()\n","\n","class NestMLP(torch.nn.Module):\n","    '''\n","        我们可以混合搭配各种组合块的方法。 \n","    '''\n","    def __init__(self):\n","        super().__init__()\n","        self.net = torch.nn.Sequential(torch.nn.Linear(20, 64), torch.nn.ReLU(),\n","                                 torch.nn.Linear(64, 32), torch.nn.ReLU())\n","        self.linear = torch.nn.Linear(32, 16)\n","\n","    def forward(self, X):\n","        return self.linear(self.net(X))\n"," \n","# 定义模型 三层卷积 一层全连接\n","class xavierNet(torch.nn.Module):\n","    def __init__(self):\n","        super(xavierNet, self).__init__()\n","        self.conv1 = torch.nn.Conv2d(1, 1, 3)\n","        print('random init:', self.conv1.weight)\n","        '''\n","            xavier 初始化方法中服从均匀分布 U(−a,a) ，分布的参数 a = gain * sqrt(6/fan_in+fan_out)，\n","            这里有一个 gain，增益的大小是依据激活函数类型来设定,该初始化方法，也称为 Glorot initialization\n","        '''\n","        torch.nn.init.xavier_uniform_(self.conv1.weight, gain=1)\n","        print('xavier_uniform_:', self.conv1.weight)\n","        '''\n","            xavier 初始化方法中服从正态分布，\n","            mean=0,std = gain * sqrt(2/fan_in + fan_out)\n","        '''\n","        torch.nn.init.xavier_normal_(self.conv1.weight, gain=1)\n","        print('xavier_uniform_:', self.conv1.weight)\n","        \n","# 定义模型 三层卷积 一层全连接\n","class heNet(torch.nn.Module):\n","    def __init__(self):\n","        super(heNet, self).__init__()\n","        self.conv1 = torch.nn.Conv2d(1, 1, 3)\n","        print('random init:', self.conv1.weight)\n","        '''\n","            此为均匀分布，U~(-bound,bound)，bound=sqrt(6/(1+a^2)*fan_in)\n","            其中，a为激活函数的负半轴的斜率，relu是0\n","            mode- 可选为fan_in或fan_out,fan_in使用正向传播，方差一致；fan_out使反向传播时，方差一致nonlinearity-可选relu和leaky_relu，默认值为：leaky_relu\n","        '''\n","        torch.nn.init.kaiming_uniform(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n","        print('kaiming_uniform:', self.conv1.weight)\n","        '''\n","            此为0均值的正态分布，N~(0,std),\n","            其中std=sqrt(2/(1+a^2)*fan_in)其中，a为激活函数的负半轴的斜率，relu是0\n","            mode- 可选为fan_in或fan_out,fan_in使正向传播时，方差一致；fan_out使反向传播时，方差一致。nonlinearity-可选relu和leaky_relu，默认值为leaky_relu.\n","        '''\n","        torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n","        print('kaiming_normal_:', self.conv1.weight)\n"," \n","# 均匀分布初始化 使值服从均匀分布U(a,b)\n","# torch.nn.init.uniform_(tensor,a=0,b=1)\n","\n","# 正态分布初始化，使值服从正态分布N(mean,std)，默认值为0，1\n","# torch.nn.init.normal_(tensor,mean=0,std=1)\n","\n","# 常数初始化，值为常数val nn.init.constant_(w,0.3)\n","# torch.nn.init.constant_(tensor,val)\n","\n","# 单位矩阵初始化，将二维tensor初始化为单位矩阵\n","# torch.nn.init.eye_(tensor)\n","\n","# 正交初始化，使得tensor是正交的\n","# torch.nn.init.orthogonal_(tensor,gain=1)\n","\n","# 稀疏初始化，从正态分布N~(0,std)中进行稀疏化，使每一个column有一部分为0,sparsity-每一个column稀疏的比例，即为0的比例\n","# torch.nn.init.sparse_(tensor,sparsity,std=0.01)\n","# **nn.init.sparse_(w,sparsity=0.1)\n","\n","# 计算增益\n","# torch.nn.init.calculate_gain(nonlinearity,param=None)\n","\n","\n","\n"," \n","if __name__ == '__main__':\n","    net = MLP()\n","    net = MySequential(torch.nn.Linear(20, 256), torch.nn.ReLU(), torch.nn.Linear(256, 10))\n","    net = FixedHiddenMLP()\n","    chimera = torch.nn.Sequential(NestMLP(), torch.nn.Linear(16, 20), FixedHiddenMLP())\n","\n","    xavierNet = xavierNet()\n","    heNet = heNet()"]}],"metadata":{"kernelspec":{"display_name":"python3.8","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":2}
