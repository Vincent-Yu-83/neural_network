{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n","X = torch.rand(size=(2, 4))\n","print(net(X))\n","\n","# 检查第二个全连接层的参数\n","print(net[2].state_dict())\n","# 提取第二层的bias信息\n","print(type(net[2].bias))\n","print(net[2].bias)\n","print(net[2].bias.data)\n","# 检查第二层weight的梯度\n","print(net[2].weight.grad == None)\n","# 提取特定层参数信息或全部参数信息\n","print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n","print(*[(name, param.shape) for name, param in net.named_parameters()])\n","# 通过层的命名访问参数值\n","print(net.state_dict()['2.bias'].data)\n","\n","# 定义块1\n","def block1():\n","    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                         nn.Linear(8, 4), nn.ReLU())\n","# 定义块2\n","def block2():\n","    net = nn.Sequential()\n","    for i in range(4):\n","        # 在这里嵌套\n","        net.add_module(f'block {i}', block1())\n","    return net\n","\n","rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n","rgnet(X)\n","print(rgnet)\n","# 打印具体位置的参数值\n","print(rgnet[0][1][0].bias.data)\n","\n","# 将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0\n","def init_normal(m):\n","    if type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0, std=0.01)\n","        # nn.init.constant_(m.weight, 1)\n","        nn.init.zeros_(m.bias)\n","net.apply(init_normal)\n","net[0].weight.data[0], net[0].bias.data[0]\n","\n","# 我们需要给共享层一个名称，以便可以引用它的参数\n","shared = nn.Linear(8, 8)\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                    shared, nn.ReLU(),\n","                    shared, nn.ReLU(),\n","                    nn.Linear(8, 1))\n","net(X)\n","# 检查参数是否相同\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n","net[2].weight.data[0, 0] = 100\n","# 确保它们实际上是同一个对象，而不只是有相同的值\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 给网络初始化参数\n","class MyLinear(nn.Module):\n","    def __init__(self, in_units, units):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.randn(in_units, units))\n","        self.bias = nn.Parameter(torch.randn(units,))\n","    def forward(self, X):\n","        linear = torch.matmul(X, self.weight.data) + self.bias.data\n","        return F.relu(linear)\n","\n","linear = MyLinear(5, 3)\n","print(linear.weight)\n","print(linear(torch.rand(2, 5)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 保存和读取参数\n","x = torch.arange(4)\n","torch.save(x, 'x-file')\n","x2 = torch.load('x-file')\n","print(x2)\n","\n","y = torch.zeros(4)\n","torch.save([x, y],'x-files')\n","x2, y2 = torch.load('x-files')\n","print((x2, y2))\n","# 创建字典\n","mydict = {'x': x, 'y': y}\n","torch.save(mydict, 'mydict')\n","mydict2 = torch.load('mydict')\n","print(mydict2)\n","\n","# 存储和读取模型参数\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.hidden = nn.Linear(20, 256)\n","        self.output = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        return self.output(F.relu(self.hidden(x)))\n","\n","net = MLP()\n","X = torch.randn(size=(2, 20))\n","Y = net(X)\n","# 存储模型参数，文件名为mlp.params\n","torch.save(net.state_dict(), 'mlp.params')\n","# 初始化模型并读取参数\n","clone = MLP()\n","clone.load_state_dict(torch.load('mlp.params'))\n","clone.eval()\n","# 两个实例有相同的参数\n","Y_clone = clone(X)\n","Y_clone == Y"]}],"metadata":{"kernelspec":{"display_name":"python3.8","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":2}
