{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tools import tool_pytorch_017 as ty\n",
    "\n",
    "\n",
    "ty.set_figsize()\n",
    "content_img = ty.Image.open('../data_sets/rainier.jpg')\n",
    "ty.plt.imshow(content_img);\n",
    "\n",
    "style_img = ty.Image.open('../data_sets/autumn-oak.jpg')\n",
    "ty.plt.imshow(style_img);\n",
    "\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    '''预处理函数preprocess对输入图像在RGB三个通道分别做标准化，并将结果变换成卷积神经网络接受的输入格式。'''\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "    '''后处理函数postprocess则将输出图像中的像素值还原回标准化之前的值。 \n",
    "    由于图像打印函数要求每个像素的浮点数值在0～1之间，我们对小于0和大于1的值分别取0和1。'''\n",
    "    img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))\n",
    "\n",
    "# 抽取图像特征，基于ImageNet数据集预训练的VGG-19模型\n",
    "pretrained_net = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# 选择VGG网络中某些层的输出。 一般来说，越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息。\n",
    "style_layers, content_layers = [0, 5, 10, 19, 28], [25]\n",
    "\n",
    "# 构建一个新的网络net，它只保留需要用到的VGG的所有层。\n",
    "net = nn.Sequential(*[pretrained_net.features[i] for i in\n",
    "                      range(max(content_layers + style_layers) + 1)])\n",
    "\n",
    "def extract_features(X, content_layers, style_layers):\n",
    "    '''给定输入X，如果我们简单地调用前向传播net(X)，只能获得最后一层的输出。 \n",
    "    由于我们还需要中间层的输出，因此这里我们逐层计算，并保留内容层和风格层的输出。'''\n",
    "    contents = []\n",
    "    styles = []\n",
    "    for i in range(len(net)):\n",
    "        X = net[i](X)\n",
    "        if i in style_layers:\n",
    "            styles.append(X)\n",
    "        if i in content_layers:\n",
    "            contents.append(X)\n",
    "    return contents, styles\n",
    "\n",
    "def get_contents(image_shape, device):\n",
    "    '''对内容图像抽取内容特征'''\n",
    "    content_X = preprocess(content_img, image_shape).to(device)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape, device):\n",
    "    '''对风格图像抽取风格特征'''\n",
    "    style_X = preprocess(style_img, image_shape).to(device)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y\n",
    "\n",
    "def content_loss(Y_hat, Y):\n",
    "    # 我们从动态计算梯度的树中分离目标：\n",
    "    # 这是一个规定的值，而不是一个变量。\n",
    "    return torch.square(Y_hat - Y.detach()).mean()\n",
    "\n",
    "def gram(X):\n",
    "    '''通过平方误差函数衡量合成图像与风格图像在风格上的差异。'''\n",
    "    num_channels, n = X.shape[1], X.numel() // X.shape[1]\n",
    "    X = X.reshape((num_channels, n))\n",
    "    return torch.matmul(X, X.T) / (num_channels * n)\n",
    "\n",
    "\n",
    "def style_loss(Y_hat, gram_Y):\n",
    "    '''风格损失的平方误差函数'''\n",
    "    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()\n",
    "\n",
    "def tv_loss(Y_hat):\n",
    "    '''全变分损失，主要用于图像降噪'''\n",
    "    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
    "                  torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())\n",
    "    \n",
    "content_weight, style_weight, tv_weight = 1, 1e3, 10\n",
    "\n",
    "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
    "    # 分别计算内容损失、风格损失和全变分损失\n",
    "    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
    "        contents_Y_hat, contents_Y)]\n",
    "    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
    "        styles_Y_hat, styles_Y_gram)]\n",
    "    tv_l = tv_loss(X) * tv_weight\n",
    "    # 对所有损失求和\n",
    "    l = sum(10 * styles_l + contents_l + [tv_l])\n",
    "    return contents_l, styles_l, tv_l, l\n",
    "\n",
    "class SynthesizedImage(nn.Module):\n",
    "    '''将合成的图像视为模型参数。模型的前向传播只需返回模型参数即可。'''\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = nn.Parameter(torch.rand(*img_shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight\n",
    "    \n",
    "def get_inits(X, device, lr, styles_Y):\n",
    "    '''创建了合成图像的模型实例，并将其初始化为图像X。'''\n",
    "    gen_img = SynthesizedImage(X.shape).to(device)\n",
    "    gen_img.weight.data.copy_(X.data)\n",
    "    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer\n",
    "        \n",
    "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
    "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\n",
    "    animator = ty.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.zero_grad()\n",
    "        contents_Y_hat, styles_Y_hat = extract_features(\n",
    "            X, content_layers, style_layers)\n",
    "        contents_l, styles_l, tv_l, l = compute_loss(\n",
    "            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X))\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X\n",
    "\n",
    "# 首先将内容图像和风格图像的高和宽分别调整为300和450像素，用内容图像来初始化合成图像。\n",
    "device, image_shape = ty.try_gpu(), (300, 450)\n",
    "net = net.to(device)\n",
    "content_X, contents_Y = get_contents(image_shape, device)\n",
    "_, styles_Y = get_styles(image_shape, device)\n",
    "output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
