{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net2(\n",
      "  (mlp): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n",
      "[epoch=1, batch_index=300, training_loss=1.2443753224611283]\n",
      "[epoch=1, batch_index=600, training_loss=0.8191303363442421]\n",
      "Accuracy on test set:79.95\n",
      "[epoch=2, batch_index=300, training_loss=0.7394830443461736]\n",
      "[epoch=2, batch_index=600, training_loss=0.7124705464641253]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 226\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m    225\u001b[0m     train(epoch, train_loader, net, loss_func, optimizer)\n\u001b[0;32m--> 226\u001b[0m     ah \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# 调整学习率\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# scheduler1.step()\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# scheduler_warmup.step()  \u001b[39;00m\n\u001b[1;32m    230\u001b[0m     accuracyHistory\u001b[38;5;241m.\u001b[39mappend(ah)\n",
      "Cell \u001b[0;32mIn[1], line 160\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(test_loader, model)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# 验证集合不需要进行反向传播，只需要进行正向，所以用 no_grad() 来取消梯度\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    161\u001b[0m         images,labels\u001b[38;5;241m=\u001b[39mdata\n\u001b[1;32m    162\u001b[0m         outputs\u001b[38;5;241m=\u001b[39mmodel(images)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.8/lib/python3.8/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "# 加载mnist.pkl.gz\n",
    "def load_data_pickle(data_path, filename, batchSize):\n",
    "    # 解压缩数据集\n",
    "    with gzip.open((PATH/FILENAME).as_posix(),\"rb\") as f:\n",
    "        ((x_train,y_train),(x_valid,y_valid),(x_test,y_test))=pickle.load(f,encoding=\"iso-8859-1\")\n",
    "    #x_train(50000,784),y_train(50000,) x_valid(10000, 784),y_valid(10000,) x_test(10000, 784),y_test(10000,)\n",
    "    #=================数据转为tensor才能参与建模训练===\n",
    "    x_train,y_train,x_valid,y_valid,x_test,y_test=map(torch.tensor, (x_train,y_train,x_valid,y_valid,x_test,y_test))\n",
    "    train_loader = DataLoader(TensorDataset(x_train,y_train), \n",
    "                              shuffle=True, \n",
    "                              batch_size=batchSize, \n",
    "                              drop_last=True)\n",
    "    valid_loader = DataLoader(TensorDataset(x_valid,y_valid), \n",
    "                              shuffle=True, \n",
    "                              batch_size=batchSize, \n",
    "                              drop_last=True)\n",
    "    test_loader = DataLoader(TensorDataset(x_test,y_test), \n",
    "                             shuffle=True, \n",
    "                             batch_size=batchSize, \n",
    "                              drop_last=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "# 加载mnist datasets\n",
    "def load_data(data_path, batchSize):\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #均值=0.1307，标准差=0.3081\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ])\n",
    "    trainset=datasets.MNIST(root=data_path, \n",
    "                            train=True, \n",
    "                            download=True, \n",
    "                            transform=transform)\n",
    "    test_dataset=datasets.MNIST(root=data_path, \n",
    "                                train=False, \n",
    "                                download=True, \n",
    "                                transform=transform)\n",
    "    \n",
    "    # 将训练集分为训练和验证集\n",
    "    train_size = int(0.8 * len(trainset))\n",
    "    valid_size = len(trainset) - train_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
    "    \n",
    "    train_loader=DataLoader(train_dataset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batchSize, \n",
    "                            drop_last=True)\n",
    "    valid_loader=DataLoader(valid_dataset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batchSize, \n",
    "                            drop_last=True)\n",
    "    test_loader=DataLoader(test_dataset, \n",
    "                           shuffle=True, \n",
    "                           batch_size=batchSize, \n",
    "                            drop_last=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "# 验证数据集，随机显示数字\n",
    "def showRandomPictures(data_loader):\n",
    "    data_iter=iter(data_loader)\n",
    "    images,labels=data_iter.__next__()\n",
    "    # Choose a random index from the batch\n",
    "    random_index = random.randint(0, batchSize - 1)\n",
    " \n",
    "    # Display the transformed image\n",
    "    transformed_image = images[random_index].squeeze().numpy()\n",
    "    transformed_image = (transformed_image * 0.3081) + 0.1307  # Inverse normalization\n",
    "    plt.imshow(transformed_image, cmap='gray')  # Assuming MNIST images are grayscale\n",
    "    plt.title(f\"Label: {labels[random_index].item()}\")\n",
    "    plt.show() \n",
    "\n",
    "# 用序列容器创建模型架构\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, function) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            # 序列容器可以对每层进行命名\n",
    "            self.mlp.add_module(\n",
    "                name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            self.mlp.add_module(name=\"A{:d}\".format(i), module=function)\n",
    "\n",
    "        # self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入的图像变为1行784列的向量，通俗的说就是将一张28 ✖ 28的图像的所有像素值拼起来，-1表示自动计算N的值（N表示样本数量）\n",
    "        x = x.view(-1,784)\n",
    "        x = self.mlp(x)\n",
    "        # x=self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# 使用模型数组创建模型架构\n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, function):\n",
    "        super(Net2, self).__init__()\n",
    "        self.mlp = nn.ModuleList()\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            self.mlp.append(nn.Linear(in_size, out_size))\n",
    "            self.mlp.append(function)\n",
    "        \n",
    "        # self.dropout=nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入的图像变为1行784列的向量，通俗的说就是将一张28 ✖ 28的图像的所有像素值拼起来，-1表示自动计算N的值（N表示样本数量）\n",
    "        x = x.view(-1,784)\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for i, l in enumerate(self.mlp):\n",
    "            x = l(x)\n",
    "        \n",
    "        # x=self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(epoch, train_loader, model, loss_func, optimizer):\n",
    "    running_loss=0.0\n",
    "    for batch_idx,data in enumerate(train_loader,0):\n",
    "        # 将模型设置为训练模式。当模型包含 Batch Normalization 和 Dropout 等层时，这个方法确保这些层在训练阶段能够正常工作。\n",
    "        model.train() #更新w和b\n",
    "        inputs,target=data\n",
    "        # 在每次调用正向传播之后需要optimizer.zero_grad()进行梯度清零操作，不然梯度就会不断累加。\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        outputs=model(inputs)\n",
    "        loss=loss_func(outputs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #running_loss 是用来记录累计的loss值，累加loss直接用loss.item()，不然的话就是计算图，用item取出就是数值。\n",
    "        running_loss+=loss.item()\n",
    "        if batch_idx%300==299:\n",
    "            print(f\"[epoch={epoch+1}, batch_index={batch_idx+1}, training_loss={running_loss/300}]\")\n",
    "            running_loss=0.0\n",
    "\n",
    "\n",
    "def test(test_loader, model):\n",
    "    correct=0\n",
    "    total=0\n",
    "    model.eval() #不更新w和b\n",
    "    # 验证集合不需要进行反向传播，只需要进行正向，所以用 no_grad() 来取消梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images,labels=data\n",
    "            outputs=model(images)\n",
    "            #每一行的最大值的下标[max,maxIndex] dim = 1 表示行，dim = 0表示列\n",
    "            _,predicted=torch.max(outputs.data,dim=1)\n",
    "            #label.size(0)是batch_size 表示取元组的第一个数，这里labels.size(0)返回的是N\n",
    "            total+=labels.size(0)\n",
    "            correct+=(predicted==labels).sum().item()\n",
    "    print(f\"Accuracy on test set:{100*correct/total}\")\n",
    "    return 100*correct/total\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # 定义数据集文件目录\n",
    "    DATA_PATH=Path(\"/root/projects/yesu/deep_learning/Neural_Network/data_sets\")\n",
    "    PATH=DATA_PATH / \"\"\n",
    "    FILENAME=\"mnist.pkl.gz\"\n",
    "    \n",
    "    #=====torch.nn.functional==========\n",
    "    # 交叉熵函数\n",
    "    loss_func = F.cross_entropy\n",
    "    # loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义感知机形状\n",
    "    layer_sizes = [784, 30, 10]\n",
    "    # layer_sizes = [784, 512, 256, 128, 64, 10]\n",
    "    \n",
    "    # 激活函数\n",
    "    function = torch.nn.ReLU()\n",
    "    # 初始化多层感知机\n",
    "    net=Net2(layer_sizes, function)\n",
    "    print(net)\n",
    "    \n",
    "    # 使用SGD算法构建梯度更新对象\n",
    "    # 更新net里所有参数\n",
    "    # 定义学习率\n",
    "    # 定义动量因子和权重衰减\n",
    "    optimizer=optim.SGD(net.parameters(), lr=0.01, momentum=0.5, weight_decay=5e-4)\n",
    "    \n",
    "    # 每16个epoch之后，lr变为原来的0.2\n",
    "    # scheduler1 = lr_scheduler.StepLR(optimizer,step_size=16,gamma=0.2)  \n",
    "    # scheduler1 = lr_scheduler.ExponentialLR(optimizer, gamma=0.95) # 每个epoch，lr变为原来的0.95\n",
    "    # scheduler1 = lr_scheduler.MultistepLR(optimizer,milestones=[10,20],gamma=0.5)# 10epoch后变为原来0.5，20 epoch后又衰减0.5\n",
    "    # 查看学习率\n",
    "    # scheduler1.get_lr()[0]\n",
    "    \n",
    "    # 前5个epoch，从0.001增加到0.04，之后按scheduler1变化lr\n",
    "    # scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=40, total_epoch=5, after_scheduler=scheduler1)\n",
    "    # 查看学习率\n",
    "    # scheduler_warmup.get_lr()[0]\n",
    "    \n",
    "    # 小批量数据集大小\n",
    "    batchSize=64\n",
    "    # 生成小批量数据集\n",
    "    train_loader, valid_loader, test_loader = load_data(PATH, batchSize)\n",
    "    # train_loader, valid_loader, test_loader = load_data_pickle(PATH, FILENAME, batchSize)\n",
    "    # showRandomPictures(train_loader)\n",
    "    #打印定义好的名字和w和b\n",
    "    # for name,parameter in net.named_parameters():\n",
    "    #     print(name,parameter,parameter.size())\n",
    "    \n",
    "    accuracyHistory=[]\n",
    "    for epoch in range(50):\n",
    "        train(epoch, train_loader, net, loss_func, optimizer)\n",
    "        ah = test(test_loader, net)\n",
    "        # 调整学习率\n",
    "        # scheduler1.step()\n",
    "        # scheduler_warmup.step()  \n",
    "        accuracyHistory.append(ah)\n",
    "    plt.plot(accuracyHistory)\n",
    "    plt.show()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
